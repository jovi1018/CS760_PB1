{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import io\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:31:14.385500Z","iopub.execute_input":"2021-10-11T08:31:14.386281Z","iopub.status.idle":"2021-10-11T08:31:19.231546Z","shell.execute_reply.started":"2021-10-11T08:31:14.386182Z","shell.execute_reply":"2021-10-11T08:31:19.230795Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os  \nimport cv2\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\n\n# Loading np array from images\ndef make_dataset(labels, dir_path, IMG_SIZE = 150):\n    # initial an empty list X to store image of np.array()\n    X = []\n\n    # initial an empty list Z to store labels/names of cat individauls\n    Z = []\n    for label in labels:\n        DIR = dir_path + label\n        for img in tqdm(os.listdir(DIR)):\n            path = os.path.join(DIR,img)\n            # reading images\n            img = cv2.imread(path,cv2.IMREAD_COLOR)\n            # resizing images to (150, 150, 3), 3 is the number of channels - RGB\n            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        \n            X.append(np.array(img))\n            Z.append(str(label)) \n            \n    ## Transform labels in Z to Y from class number\n    le=LabelEncoder()\n    Y=le.fit_transform(Z)\n\n    ## Transform and normalize X in the range of [0, 1]\n    X=np.array(X)\n    X=X/255.\n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:31:20.345018Z","iopub.execute_input":"2021-10-11T08:31:20.345741Z","iopub.status.idle":"2021-10-11T08:31:20.991898Z","shell.execute_reply.started":"2021-10-11T08:31:20.345706Z","shell.execute_reply":"2021-10-11T08:31:20.991142Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dir_path = '/kaggle/input/auckland-zoom-monkeys/Auckland_Zoo_Monkeys/'\n\nlabels = ['Arani', 'Inti', 'Ocuri', 'Poco', 'Rattaplan', 'Romy']\n\n# randomly select seen datset and unseen dataset\n# make open set and unseen set \ndef make_seen_unseen(labels, num_seen):\n    arr = np.arange(len(labels))\n    np.random.shuffle(arr)\n    \n    labels_seen = [labels[i] for i in arr[:num_seen]]\n    labels_unseen = [labels[i] for i in arr[num_seen:]]\n    \n    X_seen, Y_seen = make_dataset(labels_seen, dir_path)\n    X_unseen, Y_unseen = make_dataset(labels_unseen, dir_path)\n    Y_unseen = ['unseen'] * len(Y_unseen)\n    Y_unseen = np.array(Y_unseen)\n    return X_seen, Y_seen, X_unseen, Y_unseen","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:31:21.044269Z","iopub.execute_input":"2021-10-11T08:31:21.044491Z","iopub.status.idle":"2021-10-11T08:31:21.051466Z","shell.execute_reply.started":"2021-10-11T08:31:21.044466Z","shell.execute_reply":"2021-10-11T08:31:21.050536Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Configuration settings","metadata":{}},{"cell_type":"code","source":"imsize = 150\nEPOCHS = 5\nbatch_size = 256\nembeddingDim = 128\nnum_individuals = len(labels)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:31:22.364882Z","iopub.execute_input":"2021-10-11T08:31:22.365637Z","iopub.status.idle":"2021-10-11T08:31:22.373752Z","shell.execute_reply.started":"2021-10-11T08:31:22.365601Z","shell.execute_reply":"2021-10-11T08:31:22.373013Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_seen, Y_seen, X_unseen, Y_unseen = make_seen_unseen(labels, num_individuals - 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:31:26.940561Z","iopub.execute_input":"2021-10-11T08:31:26.941224Z","iopub.status.idle":"2021-10-11T08:31:58.334521Z","shell.execute_reply.started":"2021-10-11T08:31:26.941183Z","shell.execute_reply":"2021-10-11T08:31:58.333539Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Train/test set split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_seen, Y_seen, test_size = 0.2, random_state=2021)\n\nX_val_unseen, X_test_unseen, Y_val_unseen, Y_test_unseen = train_test_split(X_unseen, Y_unseen, test_size = 0.5, random_state=2021)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:33:21.693348Z","iopub.execute_input":"2021-10-11T08:33:21.693891Z","iopub.status.idle":"2021-10-11T08:33:22.041475Z","shell.execute_reply.started":"2021-10-11T08:33:21.693850Z","shell.execute_reply":"2021-10-11T08:33:22.040645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Stratified 3 fold Cross Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=3, random_state=2021, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:33:24.360666Z","iopub.execute_input":"2021-10-11T08:33:24.361173Z","iopub.status.idle":"2021-10-11T08:33:24.365507Z","shell.execute_reply.started":"2021-10-11T08:33:24.361132Z","shell.execute_reply":"2021-10-11T08:33:24.364629Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# VGG16 Classification Model","metadata":{}},{"cell_type":"code","source":"def evaluate_vgg16(lr):\n    base_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                      input_shape = (imsize,imsize,3),\n                      weights = 'imagenet')\n\n    # freeze all the layers of VGG, so they won't be trained.\n    for layer in base_model_16.layers:\n        layer.trainable = False\n\n    model_vgg_16 = tf.keras.models.Sequential([\n        base_model_16,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dense(units=num_individuals, activation='softmax')\n    ])\n\n    model_vgg_16.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='sparse_categorical_crossentropy', metrics=['acc']) \n\n    model_vgg_16.fit(x=x_train, y=y_train,\n                     epochs=EPOCHS,\n                     batch_size=batch_size,\n                     verbose=1)\n    \n    acc = model_vgg_16.evaluate(x_val, y_val)[1]\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:49:50.460014Z","iopub.execute_input":"2021-10-11T07:49:50.460591Z","iopub.status.idle":"2021-10-11T07:49:50.474679Z","shell.execute_reply.started":"2021-10-11T07:49:50.460557Z","shell.execute_reply":"2021-10-11T07:49:50.473741Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"lr = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n\nterms = {}\nfor i in lr:\n    terms['{}'.format(i)] = []\n\nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train,x_val,y_train,y_val = X_train[train_index], X_train[test_index], Y_train[train_index], Y_train[test_index]\n    \n    for i in lr:\n        terms['{}'.format(i)].append(evaluate_vgg16(lr = i))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:49:52.660086Z","iopub.execute_input":"2021-10-11T07:49:52.660760Z","iopub.status.idle":"2021-10-11T07:51:45.393111Z","shell.execute_reply.started":"2021-10-11T07:49:52.660728Z","shell.execute_reply":"2021-10-11T07:51:45.392394Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nacc = pd.DataFrame(terms)\ntable_vgg16 = acc.agg(['mean', 'std']).T\ntable_vgg16","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:51:45.397157Z","iopub.execute_input":"2021-10-11T07:51:45.397360Z","iopub.status.idle":"2021-10-11T07:51:45.440533Z","shell.execute_reply.started":"2021-10-11T07:51:45.397336Z","shell.execute_reply":"2021-10-11T07:51:45.439831Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Retrain VGG16 model on training set + val set, test it on test set","metadata":{}},{"cell_type":"code","source":"# choose best lr\nlr = 0.0001 \n\nbase_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                      input_shape = (imsize,imsize,3),\n                      weights = 'imagenet')\n\n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model_16.layers:\n    layer.trainable = False\n\nmodel_vgg_16 = tf.keras.models.Sequential([\n    base_model_16,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(units=num_individuals, activation='softmax')\n])\n\nmodel_vgg_16.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='sparse_categorical_crossentropy', metrics=['acc']) \n\nmodel_vgg_16.fit(x=X_train, y=Y_train,\n                 epochs=EPOCHS,\n                 batch_size=batch_size,\n                 verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:51:53.118520Z","iopub.execute_input":"2021-10-11T07:51:53.118791Z","iopub.status.idle":"2021-10-11T07:52:04.952452Z","shell.execute_reply.started":"2021-10-11T07:51:53.118765Z","shell.execute_reply":"2021-10-11T07:52:04.951617Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"acc_vgg16 = round(model_vgg_16.evaluate(X_test, Y_test)[1], 2)\n\nprint('Accuracy of VGG16 on the test set is {}'.format(acc_vgg16))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:52:04.954242Z","iopub.execute_input":"2021-10-11T07:52:04.954639Z","iopub.status.idle":"2021-10-11T07:52:06.884833Z","shell.execute_reply.started":"2021-10-11T07:52:04.954603Z","shell.execute_reply":"2021-10-11T07:52:06.884124Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Contrastive Loss","metadata":{}},{"cell_type":"code","source":"# Helper Function \n\n# Create positive pairs and negative pairs\nimport random\ndef create_pairs(images, labels):\n    numClasses = 44\n    # initialize two empty lists to hold the (image, image) pairs and\n    # labels to indicate if a pair is positive (0) or negative (1)\n    np.random.seed(2021)\n    pairImages = []\n    pairLabels = []\n    \n    # calculate the total number of classes present in the dataset\n    # and then build a list of indexes for each class label that\n    # provides the indexes for all examples with a given label\n    idx = [np.where(labels == i)[0] for i in range(44)]\n    \n    # loop voer all images\n    for idxA in range(len(images)):\n        # grab the current image and label belonging to the current iteration\n        currentImage = images[idxA]\n        label = labels[idxA]\n        \n        # randomly pick on an image that belongs to the *same* class label\n        posId = random.choice(idx[label])\n        posImage = images[posId]\n        \n        # prepare a positive pair and update the images and labels\n        pairImages.append([currentImage, posImage])\n        pairLabels.append([0])\n        \n        # grab the indices for each of the class labels *not* equal to\n        # the current label and randomly pick an image corresponding\n        # to a label *not* equal to the current label\n        negId = np.where(labels != label)[0]\n        negIdx = random.choice(negId)\n        negImage = images[negIdx]\n        \n        # prepare a negative pair of images and update out lists\n        pairImages.append([currentImage, negImage])\n        pairLabels.append([1])\n    \n    return (np.array(pairImages), np.array(pairLabels))\n\n\n\n# Function to calculate the distance between two images (Euclidean Distance used here)\nimport tensorflow.keras.backend as K\ndef euclidean_distance(vectors):\n    # unpack the vectors into separate lists\n    (featsA, featsB) = vectors\n    # compute the sum of squared distances between the vectors\n    sumSquared = K.sum(K.square(featsA - featsB), axis=1,\n                       keepdims=True)\n    # return the euclidean distance between the vectors\n    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n\n\n# contrastive loss function\ndef contrastive_loss(y, preds, margin=1):\n    # explicitly cast the true class label data type to the predicted\n    # class label data type (otherwise we run the risk of having two\n    # separate data types, causing TensorFlow to error out)\n    y = tf.cast(y, preds.dtype)\n    # calculate the contrastive loss between the true labels and\n    # the predicted labels\n    squaredPreds = K.square(preds)\n    squaredMargin = K.square(K.maximum(margin - preds, 0))\n    loss = K.mean((1 - y) * squaredPreds + y * squaredMargin)\n    # return the computed contrastive loss to the calling function\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:32:23.320302Z","iopub.execute_input":"2021-10-11T08:32:23.320838Z","iopub.status.idle":"2021-10-11T08:32:23.334907Z","shell.execute_reply.started":"2021-10-11T08:32:23.320792Z","shell.execute_reply":"2021-10-11T08:32:23.333809Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Closed Set","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef evaluate_cl_closed_set(lr, k):\n    base_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                      input_shape = (imsize,imsize,3),\n                                                      weights = 'imagenet')\n    \n    # freeze all the layers of VGG, so they won't be trained.\n    for layer in base_model_16.layers:\n        layer.trainable = False\n\n    model_cl = tf.keras.models.Sequential([\n        base_model_16,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n    ])\n\n    \n    imgA = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n    imgB = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n    \n    featsA = model_cl(imgA)\n    featsB = model_cl(imgB)\n   \n    distance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n    model = tf.keras.Model(inputs=[imgA, imgB], outputs=distance)\n    model.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(lr))\n\n    model.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n              batch_size = batch_size,\n              epochs=EPOCHS, \n              verbose=1)\n    \n    embedding_train_cl = []\n    for i in range(len(y_train)):\n        embedding_train_cl.append(model_cl(x_train[i].reshape(1, imsize, imsize, 3))[0])\n    embedding_train_cl = np.array(embedding_train_cl, dtype=float) \n\n    knn_cl = KNeighborsClassifier(n_neighbors = k)\n    knn_cl.fit(embedding_train_cl, y_train)\n    \n    x_test_embedding = model_cl(x_val)\n    acc = round(knn_cl.score(x_test_embedding, y_val), 2)\n    print('Accuracy on the val set with contrastive loss is {}'.format(acc))\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:52:17.499989Z","iopub.execute_input":"2021-10-11T07:52:17.500639Z","iopub.status.idle":"2021-10-11T07:52:17.617336Z","shell.execute_reply.started":"2021-10-11T07:52:17.500606Z","shell.execute_reply":"2021-10-11T07:52:17.616602Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"lr = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\nk = [1, 3, 5]\n\nterms = {}\nfor i in lr:\n    for j in k:\n        terms['{}_{}'.format(i, j)] = []\n\nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train,x_val,y_train,y_val = X_train[train_index], X_train[test_index], Y_train[train_index], Y_train[test_index]\n    \n    (pairTrain, labelTrain) = create_pairs(x_train, y_train)\n    \n    for i in lr:\n        for j in k:\n            terms['{}_{}'.format(i, j)].append(evaluate_cl_closed_set(lr = i, k = j))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T07:52:19.807826Z","iopub.execute_input":"2021-10-11T07:52:19.808111Z","iopub.status.idle":"2021-10-11T08:10:43.914232Z","shell.execute_reply.started":"2021-10-11T07:52:19.808080Z","shell.execute_reply":"2021-10-11T08:10:43.912165Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nacc = pd.DataFrame(terms)\ntable_cl_closed_set = acc.agg(['mean', 'std']).T\ntable_cl_closed_set","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:10:43.919239Z","iopub.execute_input":"2021-10-11T08:10:43.919449Z","iopub.status.idle":"2021-10-11T08:10:43.968116Z","shell.execute_reply.started":"2021-10-11T08:10:43.919424Z","shell.execute_reply":"2021-10-11T08:10:43.967284Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Retrain Contrastive Loss on trainset + val set, and test it on test set","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# choose best lr and k\nlr = 0.0001\nk = 1\n\nbase_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                  input_shape = (imsize,imsize,3),\n                                                  weights = 'imagenet')\n    \n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model_16.layers:\n    layer.trainable = False\n\nmodel_cl = tf.keras.models.Sequential([\n    base_model_16,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n])\n\n    \nimgA = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\nimgB = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n    \nfeatsA = model_cl(imgA)\nfeatsB = model_cl(imgB)\n   \ndistance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\nmodel = tf.keras.Model(inputs=[imgA, imgB], outputs=distance)\nmodel.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(lr))\n\n(pairTrain, labelTrain) = create_pairs(X_train, Y_train)\n\nmodel.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n          batch_size = batch_size,\n          epochs=EPOCHS, \n          verbose=1)\n    \nembedding_train_cl = []\nfor i in range(len(Y_train)):\n        embedding_train_cl.append(model_cl(X_train[i].reshape(1, imsize, imsize, 3))[0])\nembedding_train_cl = np.array(embedding_train_cl, dtype=float) \n\nknn_cl = KNeighborsClassifier(n_neighbors = k)\nknn_cl.fit(embedding_train_cl, Y_train)\n    \nx_test_embedding = model_cl(X_test)\nacc_cl_closed_set = round(knn_cl.score(x_test_embedding, Y_test), 2)\n\nprint('Accuracy of Constractive Loss on test set is {}'.format(round(acc_cl_closed_set, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:17:37.622219Z","iopub.execute_input":"2021-10-11T08:17:37.623013Z","iopub.status.idle":"2021-10-11T08:18:22.319585Z","shell.execute_reply.started":"2021-10-11T08:17:37.622972Z","shell.execute_reply":"2021-10-11T08:18:22.316599Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Open Set","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndef evaluate_cl_open_set(lr=0.0001, k = 1, d_t = 0.5):\n    base_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                      input_shape = (imsize,imsize,3),\n                                                      weights = 'imagenet')\n    \n    # freeze all the layers of VGG, so they won't be trained.\n    for layer in base_model_16.layers:\n        layer.trainable = False\n\n    model_cl = tf.keras.models.Sequential([\n        base_model_16,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n    ])\n\n    \n    imgA = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n    imgB = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n    \n    featsA = model_cl(imgA)\n    featsB = model_cl(imgB)\n   \n    distance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n    model = tf.keras.Model(inputs=[imgA, imgB], outputs=distance)\n    model.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(lr))\n\n    model.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n              batch_size = batch_size,\n              epochs=EPOCHS, \n              verbose=1)\n    \n    embedding_train_cl = []\n    for i in range(len(y_train)):\n        embedding_train_cl.append(model_cl(x_train[i].reshape(1, imsize, imsize, 3))[0])\n    embedding_train_cl = np.array(embedding_train_cl, dtype=float) \n\n    knn_cl = KNeighborsClassifier(n_neighbors = k)\n    knn_cl.fit(embedding_train_cl, y_train)\n    \n    #find the center point for each class in training set\n    support_cl = []\n    for i in np.unique(y_train):\n        support_cl.append(np.mean(embedding_train_cl[y_train==i], axis=0))\n        \n    support_cl = np.array(support_cl, dtype=float)\n    \n    pred = []\n    temp_x = np.append(x_val, X_val_unseen, axis=0)\n    temp_y = np.append(y_val, Y_val_unseen, axis=0)\n    \n    arr = np.arange(temp_y.shape[0])\n    np.random.shuffle(arr)\n    \n    temp_x = temp_x[arr]\n    temp_y = temp_y[arr]\n    \n    for i in range(len(temp_y)):\n        dists = []\n        for j in range(len(np.unique(y_train))):\n            embedding_test = model_cl(temp_x[i].reshape(1, 150, 150, 3))\n            embedding_anchor = support_cl[j]\n            dist = np.sum((embedding_test - embedding_anchor) ** 2) ** (1/2)\n            dists.append(dist)\n        if min(dists) >= d_t:\n            pred.append('unseen')\n        else:\n            pred.append(knn_cl.predict(embedding_test)[0])\n\n    pred = np.array(pred)\n    \n    acc_open = round(np.mean(pred == temp_y), 2)\n    print('The accuracy on the val set with Contrastive Loss is {}'.format(acc_open))\n    \n    return acc_open","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:19:10.263423Z","iopub.execute_input":"2021-10-11T08:19:10.263723Z","iopub.status.idle":"2021-10-11T08:19:10.282178Z","shell.execute_reply.started":"2021-10-11T08:19:10.263691Z","shell.execute_reply":"2021-10-11T08:19:10.281249Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"d_t = [0.4, 0.5, 0.6, 0.7, 0.8]\n\nterms = {}\nfor i in d_t:\n    terms['{}'.format(i)] = []\n\nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train,x_val,y_train,y_val = X_train[train_index], X_train[test_index], Y_train[train_index], Y_train[test_index]\n    \n    (pairTrain, labelTrain) = create_pairs(x_train, y_train)\n    \n    for i in d_t:\n        terms['{}'.format(i)].append(evaluate_cl_open_set(d_t = i))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:19:16.089554Z","iopub.execute_input":"2021-10-11T08:19:16.090341Z","iopub.status.idle":"2021-10-11T08:29:22.635395Z","shell.execute_reply.started":"2021-10-11T08:19:16.090299Z","shell.execute_reply":"2021-10-11T08:29:22.632284Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nacc = pd.DataFrame(terms)\ntable_cl_open_set = acc.agg(['mean', 'std']).T\ntable_cl_open_set","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:29:22.636473Z","iopub.status.idle":"2021-10-11T08:29:22.637018Z","shell.execute_reply.started":"2021-10-11T08:29:22.636743Z","shell.execute_reply":"2021-10-11T08:29:22.636768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retrain Contrastive Loss on trainset + val set, and test it on test set","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# choose the best value of d_t\nlr = 0.0001\nk = 1\nd_t = 0.5\n\nbase_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                  input_shape = (imsize,imsize,3),\n                                                  weights = 'imagenet')\n    \n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model_16.layers:\n    layer.trainable = False\n\nmodel_cl = tf.keras.models.Sequential([\n    base_model_16,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n])\n\nimgA = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\nimgB = tf.keras.layers.Input(shape=((imsize, imsize, 3)))\n\nfeatsA = model_cl(imgA)\nfeatsB = model_cl(imgB)\n\ndistance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n\nmodel = tf.keras.Model(inputs=[imgA, imgB], outputs=distance)\n\nmodel.compile(loss=contrastive_loss, optimizer=tf.keras.optimizers.Adam(lr))\n\n(pairTrain, labelTrain) = create_pairs(X_train, Y_train)\n\nmodel.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n        batch_size = batch_size,\n        epochs=EPOCHS, \n        verbose=1)\n\nembedding_train_cl = []\nfor i in range(len(Y_train)):\n    embedding_train_cl.append(model_cl(X_train[i].reshape(1, imsize, imsize, 3))[0])\n\nembedding_train_cl = np.array(embedding_train_cl, dtype=float) \n\nknn_cl = KNeighborsClassifier(n_neighbors = k)\nknn_cl.fit(embedding_train_cl, Y_train)\n    \n#find the center point for each class in training set\nsupport_cl = []\nfor i in np.unique(Y_train):\n    support_cl.append(np.mean(embedding_train_cl[Y_train==i], axis=0))\n        \nsupport_cl = np.array(support_cl, dtype=float)\n    \npred = []\ntemp_x = np.append(X_test, X_val_unseen, axis=0)\ntemp_y = np.append(Y_test, Y_val_unseen, axis=0)\n    \narr = np.arange(temp_y.shape[0])\nnp.random.shuffle(arr)\n    \ntemp_x = temp_x[arr]\ntemp_y = temp_y[arr]\n    \nfor i in range(len(temp_y)):\n    dists = []\n    for j in range(len(np.unique(Y_train))):\n        embedding_test = model_cl(temp_x[i].reshape(1, 150, 150, 3))\n        embedding_anchor = support_cl[j]\n        dist = np.sum((embedding_test - embedding_anchor) ** 2) ** (1/2)\n        dists.append(dist)\n    if min(dists) >= d_t:\n            pred.append('unseen')\n    else:\n        pred.append(knn_cl.predict(embedding_test)[0])\n\npred = np.array(pred)\n    \nacc_cl_open_set = round(np.mean(pred == temp_y), 2)\nprint('The accuracy on the test set with Open Dataset of Contrastive Loss is {}'.format(acc_cl_open_set))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:35:31.659808Z","iopub.execute_input":"2021-10-11T08:35:31.660117Z","iopub.status.idle":"2021-10-11T08:36:30.754677Z","shell.execute_reply.started":"2021-10-11T08:35:31.660079Z","shell.execute_reply":"2021-10-11T08:36:30.753800Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Triplet Loss","metadata":{}},{"cell_type":"markdown","source":"## Closed Set","metadata":{}},{"cell_type":"code","source":"def evaluate_tl_closed_set(lr, k):\n    base_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                      input_shape = (imsize, imsize, 3),\n                                                      weights = 'imagenet')\n\n    # freeze all the layers of VGG, so they won't be trained.\n    for layer in base_model_16.layers:\n        layer.trainable = False\n\n    model_tl = tf.keras.models.Sequential([\n        base_model_16,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n    ])\n\n    model_tl.compile(\n        optimizer=tf.keras.optimizers.Adam(lr),\n        loss=tfa.losses.TripletSemiHardLoss())\n\n    model_tl.fit(x=x_train, y= y_train,\n                 batch_size=batch_size,\n                 epochs=EPOCHS,\n                 verbose=1) \n        \n    embedding_train_tl = []\n    for i in range(len(y_train)):\n        embedding_train_tl.append(model_tl(x_train[i].reshape(1, imsize, imsize, 3))[0])\n        \n    embedding_train_tl = np.array(embedding_train_tl, dtype=float) \n\n    knn_tl = KNeighborsClassifier(n_neighbors = k)\n    knn_tl.fit(embedding_train_tl, y_train)\n    \n    x_test_embedding = model_tl(x_val)\n    acc = round(knn_tl.score(x_test_embedding, y_val), 2)\n    print('Accuracy on the the val set with Tripolet Loss is {}'.format(acc))\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:36:46.850009Z","iopub.execute_input":"2021-10-11T08:36:46.850318Z","iopub.status.idle":"2021-10-11T08:36:46.863827Z","shell.execute_reply.started":"2021-10-11T08:36:46.850285Z","shell.execute_reply":"2021-10-11T08:36:46.861588Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lr = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\nk = [1, 3, 5]\n\nterms = {}\nfor i in lr:\n    for j in k:\n        terms['{}_{}'.format(i, j)] = []\n        \nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train,x_val,y_train,y_val = X_train[train_index], X_train[test_index], Y_train[train_index], Y_train[test_index]\n    \n    for i in lr:\n        for j in k:\n            terms['{}_{}'.format(i, j)].append(evaluate_tl_closed_set(lr = i, k = j))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:36:47.322737Z","iopub.execute_input":"2021-10-11T08:36:47.322972Z","iopub.status.idle":"2021-10-11T08:46:55.971697Z","shell.execute_reply.started":"2021-10-11T08:36:47.322945Z","shell.execute_reply":"2021-10-11T08:46:55.970814Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nacc = pd.DataFrame(terms)\ntable_tl_closed_set = acc.agg(['mean', 'std']).T\ntable_tl_closed_set","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:46:55.973526Z","iopub.execute_input":"2021-10-11T08:46:55.973749Z","iopub.status.idle":"2021-10-11T08:46:56.021374Z","shell.execute_reply.started":"2021-10-11T08:46:55.973721Z","shell.execute_reply":"2021-10-11T08:46:56.020605Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Retrain Triplet Loss on trainset + val set, and test it on test set","metadata":{}},{"cell_type":"code","source":"# choose best lr and k\nlr = 0.001\nk = 1\n\nbase_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                  input_shape = (imsize, imsize, 3),\n                                                  weights = 'imagenet')\n\n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model_16.layers:\n        layer.trainable = False\n\nmodel_tl = tf.keras.models.Sequential([\n    base_model_16,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n])\n\nmodel_tl.compile(\n    optimizer=tf.keras.optimizers.Adam(lr),\n    loss=tfa.losses.TripletSemiHardLoss())\n\nmodel_tl.fit(x=X_train, y= Y_train,\n             batch_size=batch_size,\n             epochs=EPOCHS,\n             verbose=1) \n        \nembedding_train_tl = []\nfor i in range(len(Y_train)):\n    embedding_train_tl.append(model_tl(X_train[i].reshape(1, imsize, imsize, 3))[0])\n        \nembedding_train_tl = np.array(embedding_train_tl, dtype=float) \n\nknn_tl = KNeighborsClassifier(n_neighbors = k)\nknn_tl.fit(embedding_train_tl, Y_train)\n    \nx_test_embedding = model_tl(X_test)\nacc_tl_closed_set = round(knn_tl.score(x_test_embedding, Y_test), 2)\n\nprint('Accuracy of Triplet Loss on the test set is {}'.format(round(acc_tl_closed_set, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:47:37.021033Z","iopub.execute_input":"2021-10-11T08:47:37.021576Z","iopub.status.idle":"2021-10-11T08:48:06.976301Z","shell.execute_reply.started":"2021-10-11T08:47:37.021539Z","shell.execute_reply":"2021-10-11T08:48:06.975454Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Open Set","metadata":{}},{"cell_type":"code","source":"def evaluate_tl_open_set(lr = 0.001, k = 1, d_t =0.5):\n    base_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                      input_shape = (imsize, imsize, 3),\n                                                      weights = 'imagenet')\n\n    # freeze all the layers of VGG, so they won't be trained.\n    for layer in base_model_16.layers:\n        layer.trainable = False\n\n    model_tl = tf.keras.models.Sequential([\n        base_model_16,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n    ])\n\n    model_tl.compile(\n        optimizer=tf.keras.optimizers.Adam(lr),\n        loss=tfa.losses.TripletSemiHardLoss())\n\n    model_tl.fit(x=x_train, y= y_train,\n                 batch_size=batch_size,\n                 epochs=EPOCHS,\n                 verbose=1) \n        \n    embedding_train_tl = []\n    for i in range(len(y_train)):\n        embedding_train_tl.append(model_tl(x_train[i].reshape(1, imsize, imsize, 3))[0])\n        \n    embedding_train_tl = np.array(embedding_train_tl, dtype=float) \n\n    knn_tl = KNeighborsClassifier(n_neighbors = k)\n    knn_tl.fit(embedding_train_tl, y_train)\n    \n    #find the center point for each class in training set\n    support_tl = []\n    for i in np.unique(y_train):\n        support_tl.append(np.mean(embedding_train_tl[y_train==i], axis=0))\n        \n    support_tl = np.array(support_tl, dtype=float)\n    \n    pred = []\n    temp_x = np.append(x_val, X_unseen, axis=0)\n    temp_y = np.append(y_val, Y_unseen, axis=0)\n    \n    arr = np.arange(temp_y.shape[0])\n    np.random.shuffle(arr)\n    \n    temp_x = temp_x[arr]\n    temp_y = temp_y[arr]\n    \n    for i in range(len(temp_y)):\n        dists = []\n        for j in range(len(np.unique(y_train))):\n            embedding_test = model_tl(temp_x[i].reshape(1, 150, 150, 3))\n            embedding_anchor = support_tl[j]\n            dist = np.sum((embedding_test - embedding_anchor) ** 2) ** (1/2)\n            dists.append(dist)\n        if min(dists) >= d_t:\n            pred.append('unseen')\n        else:\n            pred.append(knn_tl.predict(embedding_test)[0])\n\n    pred = np.array(pred)\n    \n    acc_open = round(np.mean(pred == temp_y), 2)\n    print('The accuracy on the Open Dataset with triplet loss is {}'.format(acc_open))\n    \n    return acc_open","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:48:43.459732Z","iopub.execute_input":"2021-10-11T08:48:43.460023Z","iopub.status.idle":"2021-10-11T08:48:43.477259Z","shell.execute_reply.started":"2021-10-11T08:48:43.459990Z","shell.execute_reply":"2021-10-11T08:48:43.476536Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"d_t = [0.4, 0.5, 0.6, 0.7, 0.8]\n\nterms = {}\nfor i in d_t:\n    terms['{}'.format(i)] = []\n\nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train,x_val,y_train,y_val = X_train[train_index], X_train[test_index], Y_train[train_index], Y_train[test_index]\n    \n    for i in d_t:\n        terms['{}'.format(i)].append(evaluate_tl_open_set(d_t = i))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T08:48:49.537724Z","iopub.execute_input":"2021-10-11T08:48:49.538250Z","iopub.status.idle":"2021-10-11T09:01:45.091739Z","shell.execute_reply.started":"2021-10-11T08:48:49.538207Z","shell.execute_reply":"2021-10-11T09:01:45.090876Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nacc = pd.DataFrame(terms)\ntable_tl_open_set = acc.agg(['mean', 'std']).T\ntable_tl_open_set","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:01:45.093324Z","iopub.execute_input":"2021-10-11T09:01:45.094095Z","iopub.status.idle":"2021-10-11T09:01:45.116520Z","shell.execute_reply.started":"2021-10-11T09:01:45.094034Z","shell.execute_reply":"2021-10-11T09:01:45.115594Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Retrain Triplet Loss on trainset + val set, and test it on test set","metadata":{}},{"cell_type":"code","source":"lr = 0.001\nk = 1\nd_t = 0.6\n\nbase_model_16 = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                  input_shape = (imsize, imsize, 3),\n                                                  weights = 'imagenet')\n\n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model_16.layers:\n    layer.trainable = False\n\nmodel_tl = tf.keras.models.Sequential([\n    base_model_16,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(embeddingDim, activation=None), # No activation on final dense layer\n    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n])\n\nmodel_tl.compile(\n    optimizer=tf.keras.optimizers.Adam(lr),\n    loss=tfa.losses.TripletSemiHardLoss())\n\nmodel_tl.fit(x = X_train, y = Y_train,\n             batch_size=batch_size,\n             epochs=EPOCHS,\n             verbose=1) \n        \nembedding_train_tl = []\nfor i in range(len(Y_train)):\n    embedding_train_tl.append(model_tl(X_train[i].reshape(1, imsize, imsize, 3))[0])\n        \nembedding_train_tl = np.array(embedding_train_tl, dtype=float) \n\nknn_tl = KNeighborsClassifier(n_neighbors = k)\nknn_tl.fit(embedding_train_tl, Y_train)\n    \n#find the center point for each class in training set\nsupport_tl = []\nfor i in np.unique(Y_train):\n    support_tl.append(np.mean(embedding_train_tl[Y_train==i], axis=0))\n        \nsupport_tl = np.array(support_tl, dtype=float)\n    \npred = []\ntemp_x = np.append(X_test, X_unseen, axis=0)\ntemp_y = np.append(Y_test, Y_unseen, axis=0)\n    \narr = np.arange(temp_y.shape[0])\nnp.random.shuffle(arr)\n    \ntemp_x = temp_x[arr]\ntemp_y = temp_y[arr]\n    \nfor i in range(len(temp_y)):\n    dists = []\n    for j in range(len(np.unique(Y_train))):\n        embedding_test = model_tl(temp_x[i].reshape(1, 150, 150, 3))\n        embedding_anchor = support_tl[j]\n        dist = np.sum((embedding_test - embedding_anchor) ** 2) ** (1/2)\n        dists.append(dist)\n    if min(dists) >= d_t:\n        pred.append('unseen')\n    else:\n        pred.append(knn_tl.predict(embedding_test)[0])\n\npred = np.array(pred)\n    \nacc_tl_open_set = round(np.mean(pred == temp_y), 2)\nprint('The accuracy on the test set with Open Dataset of Triplet Loss is {}'.format(acc_tl_open_set))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:02:20.117217Z","iopub.execute_input":"2021-10-11T09:02:20.117693Z","iopub.status.idle":"2021-10-11T09:03:04.008598Z","shell.execute_reply.started":"2021-10-11T09:02:20.117655Z","shell.execute_reply":"2021-10-11T09:03:04.007801Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Summary on Closed Set","metadata":{}},{"cell_type":"code","source":"# After tuning hps (lr and k), the best accuracy on val set.\nbest_vgg16 = table_vgg16.loc['0.0001']\nbest_cl = table_cl_closed_set.loc['0.0001_1']\nbest_tl = table_tl_closed_set.loc['0.001_1']\n\npd.DataFrame({'VGG16_(LR = 0.0001)': best_vgg16,\n              'Contrastive_Loss(LR = 0.0001, k = 1)': best_cl, \n              'Triplet_Loss(LR = 0.001, k = 1)': best_tl\n             })","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:03:04.010330Z","iopub.execute_input":"2021-10-11T09:03:04.010736Z","iopub.status.idle":"2021-10-11T09:03:04.043491Z","shell.execute_reply.started":"2021-10-11T09:03:04.010699Z","shell.execute_reply":"2021-10-11T09:03:04.042384Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Accuracy on test set\n\npd.DataFrame([acc_vgg16, acc_cl_closed_set, acc_tl_closed_set], index = ['VGG16', 'Contrastive_Loss', 'Triplet_Loss'], columns = ['Accuracy']).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary on Open Set","metadata":{}},{"cell_type":"code","source":"# After tuning hps (d_t), the best accuracy on val set.\n\nbest_cl = table_cl_open_set.loc['0.5']\nbest_tl = table_tl_open_set.loc['0.6']\n\npd.DataFrame({'Contrastive_Loss(d_t = )': best_cl, \n              'Triplet_Loss(d_t = )': best_tl\n             })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy on test set\n\npd.DataFrame([acc_cl_open_set, acc_tl_open_set], index = ['Contrastive_Loss', 'Triplet_Loss'], columns = ['Accuracy']).T","metadata":{},"execution_count":null,"outputs":[]}]}